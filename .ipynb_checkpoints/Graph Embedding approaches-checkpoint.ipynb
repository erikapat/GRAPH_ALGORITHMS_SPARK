{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Embedding approaches\n",
    "\n",
    "“Graph embeddings are the transformation of property graphs to a vector or a set of vectors.” \n",
    "\n",
    "* Graph embeddings are the representation of nodes and relationships in a graph as feature vectors. These are merely collections of features that have dimensional mappings, such as the (x,y,z).\n",
    "\n",
    "* Graph embeddings are also useful for data exploration, computing similarity between entities, and reducing dimensionality to aid in statistical analysis.\n",
    "\n",
    "This is a quickly evolving space with several options, including: \n",
    "\n",
    "* node2vec, \n",
    "* struc2vec, \n",
    "* GraphSAGE, \n",
    "* DeepWalk, \n",
    "* DeepGL,\n",
    "* between others\n",
    "\n",
    "\n",
    "Graph Embeddings are needed because:\n",
    " \n",
    "* **Machine learning on graphs is limited**. Those network relationships of edges and nodescan only use a specific subset of mathematics, statistics, and machine learning, while vector spaces have a richer toolset of approaches.\n",
    "\n",
    "* **Embeddings are compressed representations**. Adjacency matrix describes connections between nodes in the graph. It is a |V| x |V| matrix, where |V| is a number of nodes in the graph. Each column and each row in the matrix present a node. Non-zero values in the matrix indicate that two nodes are connected. Using an adjacency matrix as a feature space for large graphs is almost impossible. Imagine a graph with 1M nodes and an adjacency matrix of 1M x 1M. Embeddings are more practical than the adjacency matrix since they pack node properties in a vector with a smaller dimension.\n",
    "\n",
    "* **Vector operations are simpler and faster than comparable operations on graphs**.\n",
    "\n",
    "### References: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Papers**\n",
    "\n",
    "- Node Embedding + clustering\n",
    "\n",
    "- Generate walks (node2vec, deepWalk)\n",
    "\n",
    "**DeepWalk:** uses random walks to produce embeddings. The random walk starts in a selected node then we move to the random neighbor from a current node for a defined number of steps. The method used to make predictions is skip-gram, just like in Word2vec architecture for text. Instead of running along the text corpus, DeepWalk runs along the graph to learn an embedding. The model can take a target node to predict it’s “context”, which in the case of a graph, means it’s connectivity, structural role, and node features.\n",
    "Although DeepWalk is relatively efficient with a score of O(|V|), this approach is transductive, meaning whenever a new node is added, the model must be retrained to embed and learn from the new node.\n",
    "\n",
    "- B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online Learning of Social Representations,” Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD ’14, pp. 701–710, 2014, doi: 10.1145/2623330.2623732.\n",
    "\n",
    "**Node2vec** is a modification of DeepWalk with an small difference in random walks. The difference between Node2vec and DeepWalk is subtle but significant. Node2vec features a walk bias variable α, which is parameterized by p and q. The parameter p prioritizes a breadth-first-search (BFS) procedure, while the parameter q prioritizes a depth-first-search (DFS) procedure. The decision of where to walk next is therefore influenced by probabilities 1/p or 1/q. BFS is ideal for learning local neighbors, while DFS is better for learning global variables.\n",
    "\n",
    "\n",
    "- A. Grover and J. Leskovec, “node2vec: Scalable Feature Learning for Networks,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco California USA, Aug. 2016, pp. 855–864, doi: 10.1145/2939672.2939754.\n",
    "\n",
    "\n",
    "- Y. Dong, N. V. Chawla, and A. Swami, “metapath2vec: Scalable Representation Learning for Heterogeneous Networks,” in Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax NS Canada, Aug. 2017, pp. 135–144, doi: 10.1145/3097983.3098036.\n",
    "\n",
    "- C. B. Bruss, A. Khazane, J. Rider, R. Serpe, A. Gogoglou, & K. E. Hines, «DeepTrax: Embedding Graphs of Financial Transactions», arXiv:1907.07225 [cs, stat], jul. 2019, Accessed: may 27, 2020. [Online]. Available [[Here]](http://arxiv.org/abs/1907.07225)\n",
    "\n",
    "- **Structural Deep Network Embedding (SDNE)**: It is designed so that embeddings preserve the first and the second order proximity. The first-order proximity is the local pairwise similarity between nodes linked by edges. It characterizes the local network structure. Two nodes in the network are similar if they are connected with the edge. When one paper cites other paper, it means that they address similar topics. The second-order proximity indicates the similarity of the nodes’ neighborhood structures. It captures the global network structure. If two nodes share many neighbors, they tend to be similar.\n",
    "\n",
    "- Vertex embedding approaches: LLE, Laplacian Eigenmaps, Graph Factorization, GraRep, HOPE, DNGR, GCN, LINE \n",
    "Graph embedding approaches: Patchy-san, sub2vec (embed subgraphs), WL kernel andDeep WL kernels\n",
    "\n",
    "- **Summary of methodologies**: Goyal, P., & Ferrara, E. (2018). Graph embedding techniques, applications, and performance: A survey. Knowledge-Based Systems, 151, 78-94.   \n",
    "[[Here]](https://arxiv.org/pdf/1705.02801.pdf)\n",
    "\n",
    "\n",
    "- SAGE: [[Here]](http://snap.stanford.edu/graphsage/)\n",
    "\n",
    "- Semi-Supervised Graph Classification: A Hierarchical Graph Perspective\n",
    "\n",
    "- A Tutorial on Network Embeddings: [[Here]](https://www.researchgate.net/publication/326913014_A_Tutorial_on_Network_Embeddings) \n",
    "\n",
    "\n",
    "##### **Interesting tutorials**\n",
    "\n",
    "* Graph Embedding for Deep Learning [[Here]](Ghttps://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Open challenges**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scalability:\n",
    "Graph size,\n",
    "Vocabulary size\n",
    "- Evaluation of embeddings\n",
    "- Applications of node / graph embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Community detection / graph coarsening\n",
    "* RW generation (deepwalk) & node embedding using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
